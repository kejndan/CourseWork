
	
Для вычисления оптимальных возможных значений гиперпараметров алгоритмов классификации используется 20 датасетов, таблица с ними будет приведена ниже.

\textbf{ТАБЛИЦА}

Метрикой качества здесь будет являться F-мера: $ F = \frac{2 * precision * recall}{precision+recall} $

\subsection{Дерево решений (Decision Tree)}
Рассмотрим для данного алгоритма следующий начальный набор значений:
\begin{itemize}
	\item \textit{criterion:} \{gini, entropy\}
	\item \textit{max\_depth:} [1, 29]
	\item \textit{min\_samples\_split:} [2, 29]
	\item \textit{min\_samples\_leaf:} [1, 29]
\end{itemize}
Соответственно получая 47 096 возможных комбинаций значений гиперпараметров.
\begin{figure}
	\includegraphics{DecisionTreeClassifier}
	\caption{Начальный набор значений гиперпараметров для алгоритма Decision tree classifier.}
\end{figure}

После того как мы избавимся от значений гиперпараметров, которые встречаются реже всего или имеют низкий показатель точности, получим следующие:
\begin{itemize}
	\item \textit{criterion:} \{gini, entropy\}
	\item \textit{max\_depth:} [5, 29]
	\item \textit{min\_samples\_split:} [1, 29]
	\item \textit{min\_samples\_leaf:} [1, 25]
\end{itemize}
\begin{figure}
	\includegraphics{DecisionTreeClassifier_changed}
	\caption{Итоговый набор значений гиперпараметров для алгоритма Decision tree classifier.}
\end{figure}
Теперь данный алгоритм имеет 35000 возможных комбинации значений гиперпараметров. 79\% точек осталось.

\subsection{Cлучайные и сверхслучайные деревья решения (Random Forest and Extra Trees)}
Здесь будет рассмотрено сразу два алгоритма Random Forest и Extra Trees, так как это схожие алгоритмы и начальный набор значений гиперпараметров для них был один тот же. 
Для данных алгоритмов следующий начальный набор значений:
\begin{itemize}
	\item \textit{criterion:} \{gini, entropy\}
	\item \textit{max\_features:} [0.1, 1.0], с шагом 0.05
	\item \textit{min\_samples\_split:} [2, 29]
	\item \textit{min\_samples\_leaf:} [1, 29]
\end{itemize}
Соответственно получая 30 856 возможных комбинаций значений гиперпараметров.
\begin{figure}
	\includegraphics{RandomExtraClassifier}
	\caption{Начальный набор значений гиперпараметров для алгоритмов Random forest classifier и Extra trees classifier.}
\end{figure}

После того как мы избавимся от значений гиперпараметров, которые встречаются реже всего или имеют низкий показатель точности, получим следующие возможные значения:
\begin{itemize}
	\item \textit{criterion:} \{gini, entropy\}
	\item \textit{max\_features:} [0.1, 1.0], с шагом 0.05
	\item \textit{min\_samples\_split:} [1, 29]
	\item \textit{min\_samples\_leaf:} [1, 20]
\end{itemize}
\begin{figure}
	\includegraphics{RandomExtraClassifier_changed}
	\caption{Итоговый набор значений гиперпараметров для алгоритмов  Random forest classifier и Extra trees classifier.}
\end{figure}
Теперь данные алгоритмы имеют 21 280 возможных комбинации значений гиперпараметров. 82\% точек осталось. 

\subsection{Градиентный бустинг (Gradient boosting)}
Рассмотрим для данного алгоритма следующий начальный набор значений гиперпараметров:
\begin{itemize}
	\item \textit{learning\_rate:} \{1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1\}
	\item \textit{max\_depth:} [1, 19]
	\item \textit{min\_samples\_split:} [2, 30]
	\item \textit{min\_samples\_leaf:} [1, 30]
	\item \textit{subsample:} [0.1, 1.00], с шагом 0.05
	\item \textit{max\_features:} [0.05, 1.0], с шагом 0.05
\end{itemize}
Соответственно получая 35 175 840 возможных комбинаций значений гиперпараметров.
\begin{figure}
	\includegraphics{GradientBoostingClassifier}
	\caption{Начальный набор значений гиперпараметров для алгоритма Gradient Boosting classifier.}
\end{figure}


После того как мы избавимся от значений гиперпараметров, которые встречаются реже всего или имеют низкий показатель точности, получим следующие возможные значения:
\begin{itemize}
	\item \textit{learning\_rate:} \{1e-3, 1e-2, 1e-1, 0.5\}
	\item \textit{max\_depth:} [1, 19]
	\item \textit{min\_samples\_split:} [5, 30]
	\item \textit{min\_samples\_leaf:} [1, 30]
	\item \textit{subsample:} [0.1, 1.00], с шагом 0.05
	\item \textit{max\_features:} [0.05, 1.0], с шагом 0.05
\end{itemize}
\begin{figure}
	\includegraphics{GradientBoostingClassifier_changed}
	\caption{Итоговый набор значений гиперпараметров для алгоритма для алгоритма Gradient Boosting classifier.}
\end{figure}
Теперь данный алгоритм имеет 20 938 000 возможных комбинации значений гиперпараметров. 77\% точек осталось. 

\subsection{XGBoost}
Рассмотрим для данного алгоритма следующий начальный набор значений гиперпараметров:
\begin{itemize}
	\item \textit{learning\_rate:} \{0.0001, 0.001, 0.01,, 0.1, 0.5, 1\}
	\item \textit{max\_depth:} [1, 20]
	\item \textit{subsample:} [0.05, 1.00], с шагом 0.05
	\item \textit{min\_child\_weight:} [1, 29]
\end{itemize}
Соответственно получая 66 120 возможных комбинаций значений гиперпараметров.
\begin{figure}
	\includegraphics{XGBoostClassifier}
	\caption{Начальный набор значений гиперпараметров для алгоритма XGBoost classifier.}
\end{figure}

После того как мы избавимся от значений гиперпараметров, которые встречаются реже всего или имеют низкий показатель точности, получим следующие возможные значения:
\begin{itemize}
	\item \textit{learning\_rate:} \{0.01, 0.1, 0.5, 1\}
	\item \textit{max\_depth:} [1, 20]
	\item \textit{subsample:} [0.05, 1.00], с шагом 0.05
	\item \textit{min\_child\_weight:} [1, 20]
\end{itemize}
\begin{figure}
	\includegraphics{XGBoostClassifier_changed}
	\caption{Итоговый набор значений гиперпараметров для алгоритма для алгоритма XGBoost classifier.}
\end{figure}
Теперь данный алгоритм имеет 30 400 возможных комбинации значений гиперпараметров. 70\% точек осталось.\newline

Следующие алгоритмы имеют относительно маленькое число возможных значений гиперпараметров, поэтому для них не будет выполняться поиск наиболее оптимальных возможных значений гиперепараметров.

\subsection{Метод k-ближайших соседей (k-nearest neighbors)}
Набор возможных значений для данного алгоритма является следующим:
\begin{itemize}
	\item \textit{n\_neighbors:} [1, 100]
	\item \textit{weights:} \{uniform, distance\}
	\item \textit{p:} \{1, 2\}
\end{itemize}

\subsection{Логистическая регрессия  (Logistic Regression)}
Набор возможных значений для данного алгоритма является следующим:
\begin{itemize}
	\item \textit{penalty:} \{elasticnet\}
	\item \textit{l1\_ratio:} \{0.25, 0.0, 1.0, 0.75, 0.5\}
	\item \textit{C:} \{1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1, 5, 10, 15, 20, 25, 50, 70\}
\end{itemize}

\subsection{AdaBoost}
Набор возможных значений для данного алгоритма является следующим:
\begin{itemize}
	\item \textit{learning\_rate:} \{1e-3, 1e-2, 1e-1, 0.5, 1\}
\end{itemize}

\subsection{Стохастический градиентный спуск  (Stochastic Gradient Descent)}
Набор возможных значений для данного алгоритма является следующим:
\begin{itemize}
	\item \textit{loss:} \{log, hinge, modified\_huber, squared\_hinge, perceptron\}
	\item \textit{penalty:} \{elasticnet\}
	\item \textit{alpha:} \{0.0, 0.01, 0.001\}
	\item \textit{learning\_rate:} \{invscaling, constant\}
	\item \textit{fit\_intercept:} \{True, False\}
	\item \textit{l1\_ratio:} \{0.25, 0.0, 1.0, 0.75, 0.5\}
	\item \textit{eta0:} \{0.1, 1.0, 0.01\}
	\item \textit{power\_t:} \{0.5, 0.0, 1.0, 0.1, 100, 10, 50\}
\end{itemize}


\subsection{Метод опорных векторов для классификации  (Support Vector Classification)}
Набор возможных значений для данного алгоритма является следующим:
\begin{itemize}
	\item \textit{penalty:} \{ l1, l2 \}
	\item \textit{loss:} \{ hinge, squared\_hinge \}
	\item \textit{tol:} \{1e-5, 1e-4, 1e-3, 1e-2, 1e-1\}
	\item \textit{C:} \{1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1, 5, 10, 15, 20, 25, 50, 70\}
\end{itemize}

\subsection{Наивный байсовский классификатор  (Naive Bayes)}
Здесь будут приведены различные байсовские алгоритмы классификации и также значениях их гиперпараметров:
\begin{itemize}
		\item {GaussianNB}
		\item {BernoulliNB}
				\begin{itemize}
				\item \textit{alpha:} \{ 1e-3, 1e-2, 1e-1, 1, 10, 100 \}
				\end{itemize}
		\item {MultinomialNB}
				\begin{itemize}
				\item \textit{alpha:} \{ 1e-3, 1e-2, 1e-1, 1, 10, 100 \}
				\end{itemize}
\end{itemize}

